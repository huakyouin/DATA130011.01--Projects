{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self-define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def time_me(fn):   # 用于统计函数运行时长\n",
    "  def _wrapper(*args, **kwargs):\n",
    "    start = time.perf_counter()\n",
    "    fn(*args, **kwargs)\n",
    "    print(\"%s cost %s second\"%(fn.__name__, time.perf_counter() - start))\n",
    "  return _wrapper\n",
    "\n",
    "\n",
    "def sech_square(x):\n",
    "    return np.multiply(1/np.cosh(x),1/np.cosh(x))\n",
    "\n",
    "def linearInd2Binary(ind,nLabels):\n",
    "    n = len(ind)\n",
    "    temp = -np.ones((n,nLabels))\n",
    "    for i in range(0,n):\n",
    "        temp[i,ind[i,0]-1] = 1\n",
    "    return temp\n",
    "\n",
    "def standardizeCols(M,mu=None,sigma=None):\n",
    "# function [S,mu,sigma2] = standardize(M, mu, sigma2)\n",
    "# Make each column of M be zero mean, std 1.\n",
    "# If mu, sigma2 are omitted, they are computed from M\n",
    "    if type(mu)!=np.ndarray:mu=M.mean(axis=0)\n",
    "    if type(sigma)!=np.ndarray:sigma=np.std(M,axis=0)\n",
    "    S=(M-mu)/sigma\n",
    "    return S, mu, sigma\n",
    "\n",
    "def MLPclassificationPredict(w,X,nHidden,nLabels):\n",
    "    nInstances,nVars = X.shape\n",
    "\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        hiddenWeights.append(w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order='F'))\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F')\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "    ip.append(np.dot(np.atleast_2d(X),inputWeights))  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    yhat = np.dot(fp[-1],outputWeights)  # 模型估计各类型概率 实例数*类型数（10）\n",
    "    y=np.argmax(yhat, axis=1)\n",
    "    return y\n",
    "\n",
    "def MLPclassificationLoss(w,X,y,nHidden,nLabels):\n",
    "    X=np.atleast_2d(X)\n",
    "    y=np.atleast_2d(y)\n",
    "    nInstances,nVars = X.shape\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        hiddenWeights.append(w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\"))\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F')\n",
    "\n",
    "    f = 0\n",
    "    gInput = np.zeros((inputWeights.shape))\n",
    "    gHidden=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        gHidden.append(np.zeros((hiddenWeights[h-1].shape)))\n",
    "    gOutput = np.zeros((outputWeights.shape))\n",
    "\n",
    "\n",
    "    # Compute Output\n",
    "    yhat=np.zeros((nInstances,outputWeights.shape[1]))\n",
    "    for i in range(0,nInstances):\n",
    "        ip=[]\n",
    "        fp=[]\n",
    "\n",
    "        ip.append(np.dot(np.atleast_2d(X[i,:]),inputWeights))\n",
    "        fp.append(np.tanh(ip[0]))\n",
    "        for h in range(1,len(nHidden)):\n",
    "            ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))\n",
    "            fp.append(np.tanh(ip[h]))\n",
    "        yhat[i,:] = np.dot(fp[-1],outputWeights)\n",
    "        relativeErr = yhat-y[i,:]\n",
    "        f = f + np.sum(relativeErr**2)\n",
    "        err = 2*relativeErr\n",
    "        # Output Weights\n",
    "        for c in range(0,nLabels):\n",
    "            gOutput[:,c] = gOutput[:,c]+ err[0,c]*fp[-1]\n",
    "\n",
    "        if len(nHidden) > 1:\n",
    "            # Last Layer of Hidden Weights\n",
    "            backprop=np.zeros((nLabels,outputWeights.shape[0]))\n",
    "            for c in range(0,nLabels):\n",
    "                # print(backprop[c,:].shape,sech_square(ip[-1]).shape,outputWeights[:,c].shape)\n",
    "                backprop[c,:] = err[0,c]*(np.multiply(sech_square(ip[-1]),outputWeights[:,c].T))\n",
    "                gHidden[-1] = gHidden[-1] + np.dot(fp[-2].T,np.atleast_2d(backprop[c,:]))\n",
    "            backprop = sum(backprop,1)\n",
    "\n",
    "            # Other Hidden Layers\n",
    "            for t in range(1,len(nHidden)-2):\n",
    "                h = len(nHidden)-2-t\n",
    "                backprop = np.multiply(np.dot(np.atleast_2d(backprop),hiddenWeights[h+1].T),sech_square(ip[h+1]))\n",
    "                gHidden[h]  = gHidden[h]+ np.dot(fp[h].T,backprop)\n",
    "\n",
    "            # Input Weights\n",
    "            backprop = np.multiply(np.dot(np.atleast_2d(backprop),hiddenWeights[0].T),sech_square(ip[0]))\n",
    "            gInput = gInput + np.dot(np.atleast_2d(X[i,:]).T,backprop)\n",
    "        else:\n",
    "        # Input Weights\n",
    "            for c in range(0,nLabels):\n",
    "                \n",
    "                part1=np.atleast_2d(err[0,c]*X[i,:]).T\n",
    "                part2=np.multiply(sech_square(ip[-1]),outputWeights[:,c].T)\n",
    "                gInput = gInput + np.dot(part1,part2)\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros((w.shape))\n",
    "    # 输入到第一层\n",
    "    g[0:nVars*nHidden[0]] = gInput.reshape((nVars*nHidden[0],1),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    # 第一层到最后一层\n",
    "    for h in range(1,len(nHidden)):\n",
    "        g[offset:offset+nHidden[h-1]*nHidden[h]] = gHidden[h-1].reshape((nHidden[h-1]*nHidden[h],1),order='F')\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    # 最后一层到输出\n",
    "    g[offset:offset+nHidden[-1]*nLabels] = gOutput.reshape((nHidden[-1]*nLabels,1),order='F')\n",
    "    return f,g\n",
    "\n",
    "## 向量化版的损失函数，后续改进都直接装进该函数（到task 5）\n",
    "def MLPclassificationLoss_vec(w,X,y,nHidden,nLabels,Lossmethod='square'):\n",
    "    X=np.atleast_2d(X)\n",
    "    y=np.atleast_2d(y)\n",
    "    nInstances,nVars = X.shape  # 实例个数，维度\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        hiddenWeights.append(w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\"))\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F') # lastN*ylabelN\n",
    "\n",
    "    gInput = np.zeros((inputWeights.shape))\n",
    "    gHidden=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        gHidden.append(np.zeros((hiddenWeights[h-1].shape)))\n",
    "    gOutput = np.zeros((outputWeights.shape))\n",
    "\n",
    "\n",
    "    # Compute Output\n",
    "    yhat=np.zeros((nInstances,outputWeights.shape[1]))\n",
    "    # for i in range(0,nInstances):\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "\n",
    "    ip.append(np.dot(np.atleast_2d(X),inputWeights))  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    yhat = np.dot(fp[-1],outputWeights)  # 模型估计各类型概率 实例数*类型数（10）\n",
    "    if Lossmethod=='square':\n",
    "        relativeErr = yhat-y\n",
    "        f = np.sum(relativeErr**2)\n",
    "        err = np.atleast_2d(2*relativeErr)  # 实例数*类型数 概率分布与真实的误差\n",
    "    if Lossmethod=='softmax':\n",
    "        yhat=yhat.T\n",
    "        shift_x = yhat - np.max(yhat,axis=0)\n",
    "        s=np.exp(shift_x) \n",
    "        s=(s/np.sum(s,axis=0)).T  \n",
    "        f = np.multiply(-np.log(s),y).sum() # 前向总误差  \n",
    "        err=s\n",
    "        err[y==1]-=1\n",
    "        \n",
    "    \n",
    "    gOutput = np.dot(fp[-1].T,err)/nInstances   #  lastN*(实例数*实例数)*类型数\n",
    "    err=np.atleast_2d(np.multiply(sech_square(ip[-1]),np.dot(err,outputWeights.T)))  # 实例数*lastN\n",
    "    ## n个隐藏层只有n-1片权重区\n",
    "    for h in range(len(nHidden)-2,-1,-1):\n",
    "        # hN*实例数*实例数*(h+1)N= hN*(h+1)N\n",
    "        gHidden[h]=np.dot(fp[h].T,err)/nInstances\n",
    "        # 实例数*(h+1)N*(h+1)N*hN=实例数*hN\n",
    "        err=np.atleast_2d(np.multiply(sech_square(ip[h]),np.dot(err,hiddenWeights[h].T))) \n",
    "    gInput= np.dot(X.T,err)/nInstances\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros((w.shape))\n",
    "    # 输入到第一层\n",
    "    g[0:nVars*nHidden[0]] = gInput.reshape((nVars*nHidden[0],1),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    # 第一层到最后一层\n",
    "    for h in range(1,len(nHidden)):\n",
    "        g[offset:offset+nHidden[h-1]*nHidden[h]] = gHidden[h-1].reshape((nHidden[h-1]*nHidden[h],1),order='F')\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    # 最后一层到输出\n",
    "    g[offset:offset+nHidden[-1]*nLabels] = gOutput.reshape((nHidden[-1]*nLabels,1),order='F')\n",
    "    return f,g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "m = loadmat(sys.path[0]+\"/digits.mat\")\n",
    "m2 = loadmat(sys.path[0]+\"/weight1010.mat\")\n",
    "w=m2['w']\n",
    "X=m[\"X\"]\n",
    "y=m['y']\n",
    "Xvalid=m['Xvalid']\n",
    "Xtest=m['Xtest']\n",
    "y=m['y']\n",
    "yvalid=m['yvalid']\n",
    "ytest=m['ytest']\n",
    "n,d=X.shape # rows and columns of matrix \"X\"\n",
    "nLabels=y.max()  # Maximum value of vector \"y\"\n",
    "yExpanded = linearInd2Binary(y,nLabels)\n",
    "t = Xvalid.shape[0]\n",
    "t2 = Xtest.shape[0]\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X,mu,sigma=standardizeCols(X)\n",
    "X=np.concatenate((np.ones((n,1)),X),axis=1)\n",
    "d +=1 \n",
    "\n",
    "# Make sure to apply the same transformation to the validation/test data\n",
    "Xvalid,_,_ = standardizeCols(Xvalid,mu,sigma)\n",
    "Xvalid = np.concatenate((np.ones((t,1)),Xvalid),axis=1)\n",
    "Xtest,_,_ = standardizeCols(Xtest,mu,sigma)\n",
    "Xtest = np.concatenate((np.ones((t2,1)),Xtest),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = [10]\n",
    "    \n",
    "# Count number of parameters and initialize weights 'w'\n",
    "nParams = d*nHidden[0]\n",
    "for h in range(1,len(nHidden)):\n",
    "    nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "w = np.random.randn(nParams,1) \n",
    "\n",
    "# Train with stochastic gradient\n",
    "maxIter = 100000\n",
    "stepSize = 1e-3\n",
    "for iter in range(0,maxIter): \n",
    "    \n",
    "    if (iter)%round(maxIter/20) == 0:\n",
    "        yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "        verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "        print('Training iteration = %d, validation error = %f\\n'%(iter,verr))\n",
    "    i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "    f,g = MLPclassificationLoss(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "    w = w - stepSize*g\n",
    "\n",
    "# Evaluate test error\n",
    "yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "print('Test error with final model = %f\\n'%te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testnHidden(test):\n",
    "    answer=[]\n",
    "    for nHidden in test:\n",
    "        # Count number of parameters and initialize weights 'w'\n",
    "        nParams = d*nHidden[0]\n",
    "        for h in range(1,len(nHidden)):\n",
    "            nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "        nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "        w = np.random.randn(nParams,1) \n",
    "\n",
    "        # Train with stochastic gradient\n",
    "        maxIter = 100000\n",
    "        stepSize = 1e-3\n",
    "        for iter in range(0,maxIter): \n",
    "            \n",
    "            if (iter)%round(maxIter/5) == 0:\n",
    "                yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "                verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "                print('Training iteration = %d, validation error = %f\\n'%(iter,verr))\n",
    "            i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "            f,g = MLPclassificationLoss(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "            w = w - stepSize*g\n",
    "\n",
    "        # Evaluate test error\n",
    "        yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "        te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "        print('Test error with final model = %f\\n'%te)\n",
    "        answer.append(te)\n",
    "    return answer\n",
    "\n",
    "result1=testnHidden([[10],[20],[50],[100],[200],[500]])\n",
    "result2=testnHidden([[8,8,8,8]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 2 动量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(nHidden=[10],momentum=0):\n",
    "    answer=[]\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    maxIter = 100000\n",
    "    stepSize = 1e-3\n",
    "    for iter in range(0,maxIter): \n",
    "        \n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training iteration = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "        if iter==0: w - stepSize*g\n",
    "        elif iter!=0 :w = w - stepSize*g+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('Test error with final model = %f\\n'%te)\n",
    "    answer.append(te)\n",
    "    return answer\n",
    "\n",
    "# 带动量\n",
    "\n",
    "momentum([100],0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 5.0) # 显示大小\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "x1=[1,3,5]\n",
    "y=[0.250,0.530,0.403]\n",
    "n=[0.233,0.621,0.543]\n",
    "y=[1-i for i in y]\n",
    "n=[1-i for i in n]\n",
    "unit=0.6\n",
    "ysr = ['64','64,8','10,10']   #x标签\n",
    "y0  = ['' for i in range(len(ysr))]       #空x标签\n",
    "x2=[i+unit*1.1 for i in x1]   #bias\n",
    "group_center=[i+unit/2*1.1 for i in x1]  #组中心\n",
    "center_y=[0 for i in range(len(x1))]\n",
    "plt.bar(x1, n, alpha=0.7, width=unit, color='r',label=\"raw\", tick_label=y0)\n",
    "plt.bar(x2, y, alpha=0.7, width=unit, color='g',label=\"momentum\", tick_label=y0)\n",
    "plt.bar(group_center,center_y,tick_label=ysr)\n",
    "plt.title('带惯性与不带惯性准确率对比柱状图', fontproperties=\"SimSun\")\n",
    "plt.xlabel('nHidden')\n",
    "plt.legend() # 显示图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 3 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "m = loadmat(sys.path[0]+\"/digits.mat\")\n",
    "m2 = loadmat(sys.path[0]+\"/weight1010.mat\")\n",
    "w=m2['w']\n",
    "X=m[\"X\"]\n",
    "y=m['y']\n",
    "Xvalid=m['Xvalid']\n",
    "Xtest=m['Xtest']\n",
    "y=m['y']\n",
    "yvalid=m['yvalid']\n",
    "ytest=m['ytest']\n",
    "n,d=X.shape # rows and columns of matrix \"X\"\n",
    "nLabels=y.max()  # Maximum value of vector \"y\"\n",
    "yExpanded = linearInd2Binary(y,nLabels)\n",
    "t = Xvalid.shape[0]\n",
    "t2 = Xtest.shape[0]\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X,mu,sigma=standardizeCols(X)\n",
    "X=np.concatenate((np.ones((n,1)),X),axis=1)\n",
    "d +=1 \n",
    "\n",
    "# Make sure to apply the same transformation to the validation/test data\n",
    "Xvalid,_,_ = standardizeCols(Xvalid,mu,sigma)\n",
    "Xvalid = np.concatenate((np.ones((t,1)),Xvalid),axis=1)\n",
    "Xtest,_,_ = standardizeCols(Xtest,mu,sigma)\n",
    "Xtest = np.concatenate((np.ones((t2,1)),Xtest),axis=1)\n",
    "@time_me\n",
    "def vec(nHidden=[10],momentum=0):\n",
    "    answer=[]\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    maxIter = 100000\n",
    "    stepSize = 1e-3\n",
    "    for iter in range(0,maxIter): \n",
    "        \n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training iteration = %d, validation error = %f\\n'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "        if iter!=0 :w = w - stepSize*g+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('Test error with final model = %f\\n'%te)\n",
    "    answer.append(te)\n",
    "    return answer\n",
    "@time_me\n",
    "def novec(nHidden=[10],momentum=0):\n",
    "    answer=[]\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    maxIter = 100000\n",
    "    stepSize = 1e-3\n",
    "    for iter in range(0,maxIter): \n",
    "        \n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training iteration = %d, validation error = %f\\n'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "        if iter!=0 :w = w - stepSize*g+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('Test error with final model = %f\\n'%te)\n",
    "    answer.append(te)\n",
    "    return answer\n",
    "vec([10,10],0)\n",
    "# novec([10,10],0)\n",
    "\n",
    "#vec cost 19.61295029999974 second\n",
    "#novec cost 62.16510610000114 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 4 正则与早停"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000):\n",
    "    answer=[]\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        # 早停法\n",
    "        yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "        verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "        verrList.append(verr)\n",
    "        min_verr=min(verrList)\n",
    "        if (verr-min_verr) / min_verr > 5e-2 and flag==0:\n",
    "            print(\"\\nstop at Iterations= \",iter,', validation error =',min_verr,'\\n')\n",
    "            flag=1\n",
    "            # break\n",
    "        yhat = MLPclassificationPredict(w,X,nHidden,nLabels)\n",
    "        err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "        errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            # yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            # verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    answer.append(te)\n",
    "    return (answer,errList,verrList)\n",
    "\n",
    "\n",
    "# 早停法\n",
    "_,trainErr,validErr=train([100],0,0,8000)\n",
    "\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.888000\n",
    "# Training Iterations = 1600, validation error = 0.753600\n",
    "# Training Iterations = 3200, validation error = 0.653200\n",
    "\n",
    "# stop at Iterations  4071 , validation error = 0.6388 \n",
    "\n",
    "# Training Iterations = 4800, validation error = 0.662000\n",
    "# Training Iterations = 6400, validation error = 0.637800\n",
    "# Training Iterations = 8000, validation error = 0.631000\n",
    "\n",
    "# Test error with final model = 0.624000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 5.0) # 显示大小\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "plt.plot(trainErr,label='trainErr')\n",
    "plt.plot(validErr,label='validErr')\n",
    "plt.xlabel('iters')\n",
    "plt.title('训练误差与验证误差变化')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000,recordErr=False):\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            # 验证误差\n",
    "            v_yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "            verrList.append(verr)\n",
    "            # 训练误差\n",
    "            yhat = MLPclassificationPredict(w,X,nHidden,nLabels)\n",
    "            err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "            errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "\n",
    "# 正则化\n",
    "train([100],0.5,0.01)\n",
    "# train([100],0,0,40000)\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.907600\n",
    "# Training Iterations = 8000, validation error = 0.257000\n",
    "# Training Iterations = 16000, validation error = 0.238200\n",
    "# Training Iterations = 24000, validation error = 0.250600\n",
    "# Training Iterations = 32000, validation error = 0.223000\n",
    "# Training Iterations = 40000, validation error = 0.231200\n",
    "\n",
    "# Test error with final model = 0.231000\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.897800\n",
    "# Training Iterations = 8000, validation error = 0.263800\n",
    "# Training Iterations = 16000, validation error = 0.240400\n",
    "# Training Iterations = 24000, validation error = 0.235000\n",
    "# Training Iterations = 32000, validation error = 0.242400\n",
    "# Training Iterations = 40000, validation error = 0.237400\n",
    "\n",
    "# Test error with final model = 0.246000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 5 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000,recordErr=False,Lossmethod='square'):\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            # 验证误差\n",
    "            v_yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "            verrList.append(verr)\n",
    "            # 训练误差\n",
    "            yhat = MLPclassificationPredict(w,X,nHidden,nLabels)\n",
    "            err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "            errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels,Lossmethod)\n",
    "        if iter==0:  w = w - stepSize*(g+penalty*w)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "    \n",
    "# train([100],0.5,0.01,500000,Lossmethod='softmax')\n",
    "train([100],0.5,0.01,600000)\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.899600\n",
    "# Training Iterations = 40000, validation error = 0.271800\n",
    "# Training Iterations = 80000, validation error = 0.180200\n",
    "# Training Iterations = 120000, validation error = 0.125000\n",
    "# Training Iterations = 160000, validation error = 0.091600\n",
    "# Training Iterations = 200000, validation error = 0.070400\n",
    "# Test error with final model = 0.068000\n",
    "\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.906200\n",
    "# Training Iterations = 40000, validation error = 0.216800\n",
    "# Training Iterations = 80000, validation error = 0.181600\n",
    "# Training Iterations = 120000, validation error = 0.149800\n",
    "# Training Iterations = 160000, validation error = 0.111400\n",
    "# Training Iterations = 200000, validation error = 0.080000\n",
    "# Test error with final model = 0.089000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 6 隐含层偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=m[\"X\"]\n",
    "Xvalid=m['Xvalid']\n",
    "Xtest=m['Xtest']\n",
    "X,mu,sigma=standardizeCols(X)\n",
    "Xtest,_,_ = standardizeCols(Xtest,mu,sigma)\n",
    "Xvalid,_,_ = standardizeCols(Xvalid,mu,sigma)\n",
    "\n",
    "### 修改bias的预测函数\n",
    "def MLPclassificationPredict(w,X,nHidden,nLabels,bias):\n",
    "    nInstances,nVars = X.shape\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    inputWeights[:,0]=0\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        temp=w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\")\n",
    "        temp[:,0]=0\n",
    "        hiddenWeights.append(temp)\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F') # lastN*ylabelN\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "    ip.append(np.dot(np.atleast_2d(X),inputWeights))  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    yhat = np.dot(fp[-1],outputWeights)  # 模型估计各类型概率 实例数*类型数（10）\n",
    "    y=np.argmax(yhat, axis=1)\n",
    "    return y\n",
    "\n",
    "### 修改bias的损失函数\n",
    "def MLPclassificationLoss_vec_nb(w,X,y,nHidden,nLabels,bias,Lossmethod='square'):\n",
    "    X=np.atleast_2d(X)\n",
    "    y=np.atleast_2d(y)\n",
    "    nInstances,nVars = X.shape  # 实例个数，维度\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    inputWeights[:,0]=0\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        temp=w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\")\n",
    "        temp[:,0]=0\n",
    "        hiddenWeights.append(temp)\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F') # lastN*ylabelN\n",
    "    # 初始化\n",
    "    gInput = np.zeros((inputWeights.shape))\n",
    "    gHidden=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        gHidden.append(np.zeros((hiddenWeights[h-1].shape)))\n",
    "    gOutput = np.zeros((outputWeights.shape))\n",
    "\n",
    "    # Compute Output\n",
    "    yhat=np.zeros((nInstances,outputWeights.shape[1]))\n",
    "    # for i in range(0,nInstances):\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "    temp=np.dot(np.atleast_2d(X),inputWeights)\n",
    "    temp[:,0]=bias[0]\n",
    "    ip.append(temp)  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        temp=np.dot(fp[h-1],hiddenWeights[h-1])\n",
    "        temp[:,0]=bias[h]\n",
    "        ip.append(temp)    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    yhat = np.dot(fp[-1],outputWeights)  # 模型估计各类型概率 实例数*类型数（10）\n",
    "    if Lossmethod=='square':\n",
    "        relativeErr = yhat-y\n",
    "        f = np.sum(relativeErr**2)\n",
    "        err = np.atleast_2d(2*relativeErr)  # 实例数*类型数 概率分布与真实的误差\n",
    "    if Lossmethod=='softmax':\n",
    "        yhat=yhat.T\n",
    "        shift_x = yhat - np.max(yhat,axis=0)\n",
    "        s=np.exp(shift_x) \n",
    "        s=(s/np.sum(s,axis=0)).T  \n",
    "        f = np.multiply(-np.log(s),y).sum() # 前向总误差  \n",
    "        err=s\n",
    "        err[y==1]-=1\n",
    "        \n",
    "    \n",
    "    gOutput = np.dot(fp[-1].T,err)/nInstances   ##  lastN*(实例数*实例数)*类型数\n",
    "    # bias[-1]-=gOutput[0,:].sum()\n",
    "    err=np.atleast_2d(np.multiply(sech_square(ip[-1]),np.dot(err,outputWeights.T)))  # 实例数*lastN\n",
    "    ## n个隐藏层只有n-1片权重区\n",
    "    for h in range(len(nHidden)-2,-1,-1):\n",
    "        # hN*实例数*实例数*(h+1)N= hN*(h+1)N\n",
    "        gHidden[h]=np.dot(fp[h].T,err)/nInstances\n",
    "        gHidden[h][:,0]=0\n",
    "        # bias[-1]-=1e-3*gHidden[h][0,:].sum()\n",
    "        ## 实例数*(h+1)N*(h+1)N*hN=实例数*hN\n",
    "        err=np.atleast_2d(np.multiply(sech_square(ip[h]),np.dot(err,hiddenWeights[h].T))) \n",
    "    gInput= np.dot(X.T,err)/nInstances\n",
    "    gInput[:,0]=0\n",
    "\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros((w.shape))\n",
    "    # 输入到第一层\n",
    "    g[0:nVars*nHidden[0]] = gInput.reshape((nVars*nHidden[0],1),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    # 第一层到最后一层\n",
    "    for h in range(1,len(nHidden)):\n",
    "        g[offset:offset+nHidden[h-1]*nHidden[h]] = gHidden[h-1].reshape((nHidden[h-1]*nHidden[h],1),order='F')\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    # 最后一层到输出\n",
    "    g[offset:offset+nHidden[-1]*nLabels] = gOutput.reshape((nHidden[-1]*nLabels,1),order='F')\n",
    "    return f,g\n",
    "\n",
    "def new_bias(nHidden=[10],momentum=0,penalty=0,maxIter =100000,bias=[0],recordErr=False,Lossmethod='square'):\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nHidden=[i+1 for i in nHidden]\n",
    "    nParams = X.shape[1]*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+(nHidden[h-1])*nHidden[h]\n",
    "    nParams = nParams+(nHidden[-1])*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    # b=[1 for i in nHidden]   # 各隐含层的偏置值\n",
    "    b=bias\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            if iter%10==0:\n",
    "                # 验证误差\n",
    "                v_yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels,b)\n",
    "                verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "                verrList.append(verr)\n",
    "                # 训练误差\n",
    "                yhat = MLPclassificationPredict(w,X,nHidden,nLabels,b)\n",
    "                err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "                errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels,b)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec_nb(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels,b,Lossmethod)\n",
    "        if iter==0:  w = w - stepSize*(g+penalty*w)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels,b)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "\n",
    "\n",
    "b1=new_bias([10],0,0,40000,[1],recordErr=True)\n",
    "b0=new_bias([10],0,0,40000,[0],recordErr=True)\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.922200\n",
    "# Training Iterations = 8000, validation error = 0.608800\n",
    "# Training Iterations = 16000, validation error = 0.595400\n",
    "# Training Iterations = 24000, validation error = 0.578600\n",
    "# Training Iterations = 32000, validation error = 0.549800\n",
    "# Training Iterations = 40000, validation error = 0.548800\n",
    "\n",
    "# Test error with final model = 0.531000\n",
    "\n",
    "# Training Iterations = 0, validation error = 0.919800\n",
    "# Training Iterations = 8000, validation error = 0.577800\n",
    "# Training Iterations = 16000, validation error = 0.585800\n",
    "# Training Iterations = 24000, validation error = 0.560600\n",
    "# Training Iterations = 32000, validation error = 0.548600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 5.0) # 显示大小\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "plt.plot(b1[2],label='layer with bias=1')\n",
    "plt.plot(b0[2],label='no bias')\n",
    "plt.xlabel('iters(x10)')\n",
    "plt.title('隐含层有无偏置模型验证集准确度走势')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 7 dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPclassificationLoss_vec_dropout(w,X,y,nHidden,nLabels,Lossmethod='square',drop_prob=0):\n",
    "    X=np.atleast_2d(X)\n",
    "    y=np.atleast_2d(y)\n",
    "    nInstances,nVars = X.shape  # 实例个数，维度\n",
    "    #定义遮罩\n",
    "    mask=[]\n",
    "    p=drop_prob\n",
    "    for i in nHidden:\n",
    "        temp=np.random.random((i,1))\n",
    "        temp[temp>p]=1\n",
    "        temp[temp<=p]=0    \n",
    "        mask.append(temp)\n",
    "\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    inputWeights =np.dot(np.ones((nVars,1)),mask[0].T)*inputWeights*(1/(1-p))\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    \n",
    "    for h in range(1,len(nHidden)):\n",
    "        temp =w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\")\n",
    "        temp =np.dot(mask[h-1],mask[h].T)*temp*(1/(1-p))\n",
    "        hiddenWeights.append(temp)\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F') # lastN*ylabelN\n",
    "    outputWeights=np.dot(mask[-1],np.ones((nLabels,1)).T)*outputWeights*(1/(1-p))\n",
    "\n",
    "    gInput = np.zeros((inputWeights.shape))\n",
    "    gHidden=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        gHidden.append(np.zeros((hiddenWeights[h-1].shape)))\n",
    "    gOutput = np.zeros((outputWeights.shape))\n",
    "\n",
    "\n",
    "    # Compute Output\n",
    "    yhat=np.zeros((nInstances,outputWeights.shape[1]))\n",
    "    # for i in range(0,nInstances):\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "\n",
    "    ip.append(np.dot(np.atleast_2d(X),inputWeights))  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    yhat = np.dot(fp[-1],outputWeights)  # 模型估计各类型概率 实例数*类型数（10）\n",
    "    if Lossmethod=='square':\n",
    "        relativeErr = yhat-y\n",
    "        f = np.sum(relativeErr**2)\n",
    "        err = np.atleast_2d(2*relativeErr)  # 实例数*类型数 概率分布与真实的误差\n",
    "    if Lossmethod=='softmax':\n",
    "        yhat=yhat.T\n",
    "        shift_x = yhat - np.max(yhat,axis=0)\n",
    "        s=np.exp(shift_x) \n",
    "        s=(s/np.sum(s,axis=0)).T  \n",
    "        f = np.multiply(-np.log(s),y).sum() # 前向总误差  \n",
    "        err=s\n",
    "        err[y==1]-=1\n",
    "        \n",
    "    \n",
    "    gOutput = np.dot(fp[-1].T,err)/nInstances   #  lastN*(实例数*实例数)*类型数\n",
    "    gOutput = np.dot(mask[-1],np.ones((nLabels,1)).T)*gOutput*(1/(1-p))\n",
    "    err=np.atleast_2d(np.multiply(sech_square(ip[-1]),np.dot(err,outputWeights.T)))  # 实例数*lastN\n",
    "    ## n个隐藏层只有n-1片权重区\n",
    "    for h in range(len(nHidden)-2,-1,-1):\n",
    "        # hN*实例数*实例数*(h+1)N= hN*(h+1)N\n",
    "        gHidden[h]=np.dot(fp[h].T,err)/nInstances\n",
    "        gHidden[h]=np.dot(mask[h],mask[h+1].T)*gHidden[h]*(1/(1-p))\n",
    "        # 实例数*(h+1)N*(h+1)N*hN=实例数*hN\n",
    "        err=np.atleast_2d(np.multiply(sech_square(ip[h]),np.dot(err,hiddenWeights[h].T))) \n",
    "    gInput= np.dot(X.T,err)/nInstances\n",
    "    gInput=np.dot(np.ones((nVars,1)),mask[0].T)*gInput*(1/(1-p))\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros((w.shape))\n",
    "    # 输入到第一层\n",
    "    g[0:nVars*nHidden[0]] = gInput.reshape((nVars*nHidden[0],1),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    # 第一层到最后一层\n",
    "    for h in range(1,len(nHidden)):\n",
    "        g[offset:offset+nHidden[h-1]*nHidden[h]] = gHidden[h-1].reshape((nHidden[h-1]*nHidden[h],1),order='F')\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    # 最后一层到输出\n",
    "    g[offset:offset+nHidden[-1]*nLabels] = gOutput.reshape((nHidden[-1]*nLabels,1),order='F')\n",
    "    return f,g\n",
    "\n",
    "\n",
    "\n",
    "def train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000,recordErr=False,Lossmethod='square',drop_p=0):\n",
    "    p=drop_p\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            # 验证误差\n",
    "            v_yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "            verrList.append(verr)\n",
    "            # 训练误差\n",
    "            yhat = MLPclassificationPredict(w,X,nHidden,nLabels)\n",
    "            err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "            errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec_dropout(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels,Lossmethod,p)\n",
    "        if iter==0:  w = w - stepSize*(g+penalty*w)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "    \n",
    "# train([100],0.5,0.01,200000,Lossmethod='softmax')\n",
    "p0=train([10,10],0.5,0,10000,drop_p=0,recordErr=True)\n",
    "p1=train([10,10],0.5,0,10000,drop_p=0.5,recordErr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 8.0) # 显示大小\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "plt.subplot(211)\n",
    "plt.plot(p0[1],label='train err')\n",
    "plt.plot(p0[2],label='valid err')\n",
    "plt.xlabel('iters')\n",
    "plt.title('不使用dropout准确度')\n",
    "plt.legend()\n",
    "plt.subplot(212)\n",
    "plt.plot(p1[1],label='train err')\n",
    "plt.plot(p1[2],label='valid err')\n",
    "plt.xlabel('iters')\n",
    "plt.title('使用dropout准确度')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 8 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(w,X,y,nHidden,nLabels):\n",
    "    X=np.atleast_2d(X)\n",
    "    y=np.atleast_2d(y)\n",
    "    nInstances,nVars = X.shape  # 实例个数，维度\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        hiddenWeights.append(w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\"))\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F') # lastN*ylabelN\n",
    "\n",
    "    # Compute Output\n",
    "    yhat=np.zeros((nInstances,outputWeights.shape[1]))\n",
    "    # for i in range(0,nInstances):\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "\n",
    "    ip.append(np.dot(np.atleast_2d(X),inputWeights))  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    \n",
    "    #fp为每层特征值 所以fp[-1]即公式中的F\n",
    "    F=fp[-1]\n",
    "    temp=outputWeights\n",
    "    outputWeights=np.dot(np.dot(np.linalg.inv(np.dot(F.T,F)),F.T),y)\n",
    "    lr=0.05\n",
    "    outputWeights=(1-lr)*temp+lr*outputWeights\n",
    "    w[offset:offset+nHidden[-1]*nLabels] = outputWeights.reshape((nHidden[-1]*nLabels,1),order='F')\n",
    "    return w\n",
    "    \n",
    "\n",
    "\n",
    "def MLPclassificationLoss_vec(w,X,y,nHidden,nLabels,Lossmethod='square',drop_prob=0):\n",
    "    X=np.atleast_2d(X)\n",
    "    y=np.atleast_2d(y)\n",
    "    nInstances,nVars = X.shape  # 实例个数，维度\n",
    "    #定义遮罩\n",
    "    mask=[]\n",
    "    p=drop_prob\n",
    "    for i in nHidden:\n",
    "        temp=np.random.random((i,1))\n",
    "        temp[temp>p]=1\n",
    "        temp[temp<=p]=0    \n",
    "        mask.append(temp)\n",
    "\n",
    "    # Form Weights\n",
    "    inputWeights = w[0:nVars*nHidden[0]].reshape((nVars,nHidden[0]),order='F')\n",
    "    inputWeights =np.dot(np.ones((nVars,1)),mask[0].T)*inputWeights*(1/(1-p))\n",
    "    offset = nVars*nHidden[0]\n",
    "    hiddenWeights=[]\n",
    "    \n",
    "    for h in range(1,len(nHidden)):\n",
    "        temp =w[offset:offset+nHidden[h-1]*nHidden[h]].reshape((nHidden[h-1],nHidden[h]),order=\"F\")\n",
    "        temp =np.dot(mask[h-1],mask[h].T)*temp*(1/(1-p))\n",
    "        hiddenWeights.append(temp)\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    outputWeights = w[offset:offset+nHidden[-1]*nLabels]\n",
    "    outputWeights = outputWeights.reshape((nHidden[-1],nLabels),order='F') # lastN*ylabelN\n",
    "    outputWeights=np.dot(mask[-1],np.ones((nLabels,1)).T)*outputWeights*(1/(1-p))\n",
    "\n",
    "    gInput = np.zeros((inputWeights.shape))\n",
    "    gHidden=[]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        gHidden.append(np.zeros((hiddenWeights[h-1].shape)))\n",
    "    gOutput = np.zeros((outputWeights.shape))\n",
    "\n",
    "\n",
    "    # Compute Output\n",
    "    yhat=np.zeros((nInstances,outputWeights.shape[1]))\n",
    "    # for i in range(0,nInstances):\n",
    "    ip=[]\n",
    "    fp=[]\n",
    "\n",
    "    ip.append(np.dot(np.atleast_2d(X),inputWeights))  # 进入第一个隐藏层 实例数*firstN\n",
    "    fp.append(np.tanh(ip[0]))   # 激活 实例数*firstN\n",
    "    for h in range(1,len(nHidden)):  \n",
    "        ip.append(np.dot(fp[h-1],hiddenWeights[h-1]))    #实例数*iN\n",
    "        fp.append(np.tanh(ip[h]))       #实例数*iN\n",
    "    yhat = np.dot(fp[-1],outputWeights)  # 模型估计各类型概率 实例数*类型数（10）\n",
    "    if Lossmethod=='square':\n",
    "        relativeErr = yhat-y\n",
    "        f = np.sum(relativeErr**2)\n",
    "        err = np.atleast_2d(2*relativeErr)  # 实例数*类型数 概率分布与真实的误差\n",
    "    if Lossmethod=='softmax':\n",
    "        yhat=yhat.T\n",
    "        shift_x = yhat - np.max(yhat,axis=0)\n",
    "        s=np.exp(shift_x) \n",
    "        s=(s/np.sum(s,axis=0)).T  \n",
    "        f = np.multiply(-np.log(s),y).sum() # 前向总误差  \n",
    "        err=s\n",
    "        err[y==1]-=1\n",
    "        \n",
    "    \n",
    "    gOutput = np.dot(fp[-1].T,err)/nInstances   #  lastN*(实例数*实例数)*类型数\n",
    "    gOutput = np.dot(mask[-1],np.ones((nLabels,1)).T)*gOutput*(1/(1-p))\n",
    "    err=np.atleast_2d(np.multiply(sech_square(ip[-1]),np.dot(err,outputWeights.T)))  # 实例数*lastN\n",
    "    ## n个隐藏层只有n-1片权重区\n",
    "    for h in range(len(nHidden)-2,-1,-1):\n",
    "        # hN*实例数*实例数*(h+1)N= hN*(h+1)N\n",
    "        gHidden[h]=np.dot(fp[h].T,err)/nInstances\n",
    "        gHidden[h]=np.dot(mask[h],mask[h+1].T)*gHidden[h]*(1/(1-p))\n",
    "        # 实例数*(h+1)N*(h+1)N*hN=实例数*hN\n",
    "        err=np.atleast_2d(np.multiply(sech_square(ip[h]),np.dot(err,hiddenWeights[h].T))) \n",
    "    gInput= np.dot(X.T,err)/nInstances\n",
    "    gInput=np.dot(np.ones((nVars,1)),mask[0].T)*gInput*(1/(1-p))\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros((w.shape))\n",
    "    # 输入到第一层\n",
    "    g[0:nVars*nHidden[0]] = gInput.reshape((nVars*nHidden[0],1),order='F')\n",
    "    offset = nVars*nHidden[0]\n",
    "    # 第一层到最后一层\n",
    "    for h in range(1,len(nHidden)):\n",
    "        g[offset:offset+nHidden[h-1]*nHidden[h]] = gHidden[h-1].reshape((nHidden[h-1]*nHidden[h],1),order='F')\n",
    "        offset = offset+nHidden[h-1]*nHidden[h]\n",
    "    # 最后一层到输出\n",
    "    g[offset:offset+nHidden[-1]*nLabels] = gOutput.reshape((nHidden[-1]*nLabels,1),order='F')\n",
    "    return f,g\n",
    "\n",
    "\n",
    "\n",
    "def train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000,recordErr=False,Lossmethod='square',drop_p=0,fine_tune=False):\n",
    "    p=drop_p\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            # 验证误差\n",
    "            v_yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "            verrList.append(verr)\n",
    "            # 训练误差\n",
    "            yhat = MLPclassificationPredict(w,X,nHidden,nLabels)\n",
    "            err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "            errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = MLPclassificationLoss_vec(w,X[i-1,:],yExpanded[i-1,:],nHidden,nLabels,Lossmethod,p)\n",
    "        if iter==0:  w = w - stepSize*(g+penalty*w)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    if fine_tune:\n",
    "        w=fine_tuning(w,X,yExpanded,nHidden,nLabels)\n",
    "        yhat_new = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "        te_new=np.sum(yhat_new!=(ytest-1)[:,0])/t2\n",
    "        print('\\nTest error with final model fine-tuned = %f\\n'%te_new)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "    \n",
    "train([64,32],0.5,0.01,100000,Lossmethod='softmax',fine_tune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 9 人工增加样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0) # 显示大小\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "def rotate(image,degree,crop=False):\n",
    "    image=image[1:].reshape((16,16),order='F')\n",
    "    radius = math.pi * degree / 180\n",
    "    width, height = image.shape\n",
    "    if not crop:\n",
    "        X1 = math.ceil(abs(0.5 * height * math.cos(radius) + 0.5 * width * math.sin(radius)))\n",
    "        X2 = math.ceil(abs(0.5 * height * math.cos(radius) - 0.5 * width * math.sin(radius)))\n",
    "        Y1 = math.ceil(abs(-0.5 * height * math.sin(radius) + 0.5 * width * math.cos(radius)))\n",
    "        Y2 = math.ceil(abs(-0.5 * height * math.sin(radius) - 0.5 * width * math.cos(radius)))\n",
    "        H = int(2 * max(Y1, Y2))\n",
    "        W = int(2 * max(X1, X2))\n",
    "        dstwidth = W + 1\n",
    "        dstheight = H + 1\n",
    "    if crop:\n",
    "        dstheight = height\n",
    "        dstwidth = width\n",
    "    im_new = np.ones((dstwidth,dstheight))*image.mean()\n",
    "    for i in range(dstwidth):\n",
    "        for j in range(dstheight):\n",
    "            new_i = int(\n",
    "                (i - 0.5 * dstwidth) * math.cos(radius) - (j - 0.5 * dstheight) * math.sin(radius) + 0.5 * width)\n",
    "            new_j = int(\n",
    "                (i - 0.5 * dstwidth) * math.sin(radius) + (j - 0.5 * dstheight) * math.cos(radius) + 0.5 * height)\n",
    "            if new_i >= 0 and new_i < width and new_j >= 0 and new_j < height:\n",
    "                im_new[i,j]=image[new_i, new_j]\n",
    "    # im_new.show()\n",
    "    # sub = plt.subplot(1, 2, 1)\n",
    "    # sub.set_title(\"Src Img\")\n",
    "    # plt.imshow(image)\n",
    "    # sub = plt.subplot(1, 2, 2)\n",
    "    # sub.set_title(\"Dst Img & Nearest\")\n",
    "    # plt.imshow(im_new)\n",
    "    # plt.show()\n",
    "    return np.r_[np.ones((1,1)),im_new.reshape((256,1),order='F')]  # 样本增加了一个bias\n",
    "\n",
    "# img=X[0,1:]\n",
    "# img2=rotate(img,20,crop=True)\n",
    "# print(img2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000,recordErr=False,Lossmethod='square',rotate_p=0):\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            # 验证误差\n",
    "            v_yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "            verrList.append(verr)\n",
    "            # 训练误差\n",
    "            yhat = MLPclassificationPredict(w,X,nHidden,nLabels)\n",
    "            err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "            errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = MLPclassificationPredict(w,Xvalid,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        img=X[i-1,:]\n",
    "        # 0.2概率旋转\n",
    "        if np.random.uniform(0,1)<rotate_p: img=rotate(img,np.random.uniform(-10,10),crop=True)  \n",
    "        f,g = MLPclassificationLoss_vec(w,img,yExpanded[i-1,:],nHidden,nLabels,Lossmethod)\n",
    "        if iter==0:  w = w - stepSize*(g+penalty*w)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w,Xtest,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "    \n",
    "# train([100],0.5,0.01,200000,Lossmethod='softmax')\n",
    "train([100],0.5,0.01,100000,rotate_p=0.2)\n",
    "# Test error with final model = 0.169000\n",
    "train([100],0.5,0.01,100000)\n",
    "# Test error with final model = 0.170000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 10 卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import CNN_module  as cnn\n",
    "importlib.reload(cnn)\n",
    "cnn.hello()\n",
    "def CNN_train(nHidden=[10],momentum=0,penalty=0,maxIter = 100000,recordErr=False,Lossmethod='square',kernel_size=5):\n",
    "    # Count number of parameters and initialize weights 'w'\n",
    "    nParams = kernel_size*kernel_size\n",
    "    k=kernel_size\n",
    "    nParams += d*nHidden[0]\n",
    "    for h in range(1,len(nHidden)):\n",
    "        nParams = nParams+nHidden[h-1]*nHidden[h]\n",
    "    nParams = nParams+nHidden[len(nHidden)-1]*nLabels\n",
    "    w = np.random.randn(nParams,1) \n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    stepSize = 1e-3\n",
    "    verrList=[]\n",
    "    errList=[]\n",
    "    flag=0   # 用于标记早停点\n",
    "    for iter in range(0,maxIter+1): \n",
    "        if recordErr:\n",
    "            # 验证误差\n",
    "            v_yhat = cnn.CNN_predict(w,Xvalid,k,nHidden,nLabels)\n",
    "            verr=np.sum(v_yhat!=(yvalid-1)[:,0])/t\n",
    "            verrList.append(verr)\n",
    "            # 训练误差\n",
    "            yhat = cnn.CNN_predict(w,X,k,nHidden,nLabels)\n",
    "            err=np.sum(yhat!=(y-1)[:,0])/t\n",
    "            errList.append(err)\n",
    "        if (iter)%round(maxIter/5) == 0:\n",
    "            yhat = cnn.CNN_predict(w,Xvalid,k,nHidden,nLabels)\n",
    "            verr=np.sum(yhat!=(yvalid-1)[:,0])/t\n",
    "            print('Training Iterations = %d, validation error = %f'%(iter,verr))\n",
    "        i = math.ceil(np.random.uniform(0, n, 1)[0])\n",
    "        f,g = cnn.CNN_loss(w,X[i-1,:],yExpanded[i-1,:],k,nHidden,nLabels,Lossmethod)\n",
    "        if iter==0:  w = w - stepSize*(g+penalty*w)\n",
    "        if iter!=0 :w = w - stepSize*(g+penalty*w)+momentum*(w-last)\n",
    "        last=w\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = cnn.CNN_predict(w,Xtest,k,nHidden,nLabels)\n",
    "    te=np.sum(yhat!=(ytest-1)[:,0])/t2\n",
    "    print('\\nTest error with final model = %f\\n'%te)\n",
    "    testErr_final=te\n",
    "    if recordErr:\n",
    "        return (testErr_final,errList,verrList)\n",
    "    else:\n",
    "        return testErr_final\n",
    "\n",
    "CNN_train([100],0.5,0.01,100000,kernel_size=5,Lossmethod='softmax')\n",
    "# Training Iterations = 0, validation error = 0.926400\n",
    "# Training Iterations = 20000, validation error = 0.342000\n",
    "# Training Iterations = 40000, validation error = 0.250800\n",
    "# Training Iterations = 60000, validation error = 0.212200\n",
    "# Training Iterations = 80000, validation error = 0.199200\n",
    "# Training Iterations = 100000, validation error = 0.190800\n",
    "# Test error with final model = 0.180000\n",
    "\n",
    "\n",
    "# train([100,32],0,0,100000,Lossmethod='softmax')\n",
    "# Training Iterations = 0, validation error = 0.918000\n",
    "# Training Iterations = 20000, validation error = 0.549600\n",
    "# Training Iterations = 40000, validation error = 0.449400\n",
    "# Training Iterations = 60000, validation error = 0.405800\n",
    "# Training Iterations = 80000, validation error = 0.379800\n",
    "# Training Iterations = 100000, validation error = 0.361400\n",
    "# Test error with final model = 0.378000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
